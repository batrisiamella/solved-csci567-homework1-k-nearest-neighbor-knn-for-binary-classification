Download Link: https://assignmentchef.com/product/solved-csci567-homework1-k-nearest-neighbor-knn-for-binary-classification
<br>
<span class="kksr-muted">Rate this product</span>

<h2 id="Problem-1:-K-nearest-neighbor-(KNN)-for-binary-classification-(50-points)">Problem 1: K-nearest neighbor (KNN) for binary classification</h2>

<h4 id="Some-notes">Some notes</h4>

In this task, we will use four distance functions: (we removed the vector symbol for simplicity)

<ul>

 <li>Euclidean distance:</li>

 <li>Minkowski distance:</li>

 <li>Inner product distance:</li>

 <li>Gaussian kernel distance:</li>

 <li>Cosine Similarity:</li>

</ul>

<strong>Cosine Distance</strong> = 1 – Cosine Similarity

<strong>F1-score</strong> is a important metric for binary classification, as sometimes the accuracy metric has the false positive (a good example is in MLAPP book 2.2.3.1 “Example: medical diagnosis”, Page 29). We have provided a basic definition. For more you can read 5.7.2.3 from MLAPP book.

<img decoding="async" data-src="F1Score.png" class="lazyload" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">

 <noscript>

  <img decoding="async" src="F1Score.png">

 </noscript>

<span id="MathJax-Element-1-Frame" class="MathJax" style="box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 14px; text-indent: 0px; text-align: center; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;" tabindex="0" role="presentation" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msqrt><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E8;</mo><mi>x</mi><mo>&amp;#x2212;</mo><mi>y</mi><mo>,</mo><mi>x</mi><mo>&amp;#x2212;</mo><mi>y</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E9;</mo></msqrt></math>"><span id="MathJax-Span-1" class="math"><span id="MathJax-Span-2" class="mrow"><span id="MathJax-Span-3" class="mi">d</span><span id="MathJax-Span-4" class="mo">(</span><span id="MathJax-Span-5" class="mi">x</span><span id="MathJax-Span-6" class="mo">,</span><span id="MathJax-Span-7" class="mi">y</span><span id="MathJax-Span-8" class="mo">)</span><span id="MathJax-Span-9" class="mo">=</span><span id="MathJax-Span-10" class="msqrt"><span id="MathJax-Span-11" class="mrow"><span id="MathJax-Span-12" class="mo">⟨</span><span id="MathJax-Span-13" class="mi">x</span><span id="MathJax-Span-14" class="mo">−</span><span id="MathJax-Span-15" class="mi">y</span><span id="MathJax-Span-16" class="mo">,</span><span id="MathJax-Span-17" class="mi">x</span><span id="MathJax-Span-18" class="mo">−</span><span id="MathJax-Span-19" class="mi">y</span><span id="MathJax-Span-20" class="mo">⟩</span></span>−−−−−−−−−−−√</span></span></span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation">d(x,y)=⟨x−y,x−y⟩</span></span>

<span id="MathJax-Element-2-Frame" class="MathJax" style="box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 14px; text-indent: 0px; text-align: center; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;" tabindex="0" role="presentation" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>(</mo></mrow><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>|</mo></mrow><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>|</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>3</mn></mrow></msup><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mn>3</mn></mrow></msup></math>"><span id="MathJax-Span-21" class="math"><span id="MathJax-Span-22" class="mrow"><span id="MathJax-Span-23" class="mi">d</span><span id="MathJax-Span-24" class="mo">(</span><span id="MathJax-Span-25" class="mi">x</span><span id="MathJax-Span-26" class="mo">,</span><span id="MathJax-Span-27" class="mi">y</span><span id="MathJax-Span-28" class="mo">)</span><span id="MathJax-Span-29" class="mo">=</span><span id="MathJax-Span-30" class="texatom"><span id="MathJax-Span-31" class="mrow"><span id="MathJax-Span-32" class="mo">(</span></span></span><span id="MathJax-Span-33" class="munderover"><span id="MathJax-Span-34" class="mo">∑</span><span id="MathJax-Span-35" class="texatom"><span id="MathJax-Span-36" class="mrow"><span id="MathJax-Span-37" class="mi">i</span><span id="MathJax-Span-38" class="mo">=</span><span id="MathJax-Span-39" class="mn">1</span></span></span><span id="MathJax-Span-40" class="texatom"><span id="MathJax-Span-41" class="mrow"><span id="MathJax-Span-42" class="mi">n</span></span></span></span><span id="MathJax-Span-43" class="texatom"><span id="MathJax-Span-44" class="mrow"><span id="MathJax-Span-45" class="mo">∣∣</span></span></span><span id="MathJax-Span-46" class="msubsup"><span id="MathJax-Span-47" class="mi">x</span><span id="MathJax-Span-48" class="texatom"><span id="MathJax-Span-49" class="mrow"><span id="MathJax-Span-50" class="mi">i</span></span></span></span><span id="MathJax-Span-51" class="mo">−</span><span id="MathJax-Span-52" class="msubsup"><span id="MathJax-Span-53" class="mi">y</span><span id="MathJax-Span-54" class="texatom"><span id="MathJax-Span-55" class="mrow"><span id="MathJax-Span-56" class="mi">i</span></span></span></span><span id="MathJax-Span-57" class="msubsup"><span id="MathJax-Span-58" class="texatom"><span id="MathJax-Span-59" class="mrow"><span id="MathJax-Span-60" class="mo">∣∣</span></span></span><span id="MathJax-Span-61" class="texatom"><span id="MathJax-Span-62" class="mrow"><span id="MathJax-Span-63" class="mn">3</span></span></span></span><span id="MathJax-Span-64" class="msubsup"><span id="MathJax-Span-65" class="texatom"><span id="MathJax-Span-66" class="mrow"><span id="MathJax-Span-67" class="mo">)</span></span></span><span id="MathJax-Span-68" class="texatom"><span id="MathJax-Span-69" class="mrow"><span id="MathJax-Span-70" class="mn">1</span><span id="MathJax-Span-71" class="texatom"><span id="MathJax-Span-72" class="mrow"><span id="MathJax-Span-73" class="mo">/</span></span></span><span id="MathJax-Span-74" class="mn">3</span></span></span></span></span></span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation">d(x,y)=(∑i=1n|xi−yi|3)1/3</span></span>

<span id="MathJax-Element-3-Frame" class="MathJax" style="box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 14px; text-indent: 0px; text-align: center; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;" tabindex="0" role="presentation" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E8;</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E9;</mo></math>"><span id="MathJax-Span-75" class="math"><span id="MathJax-Span-76" class="mrow"><span id="MathJax-Span-77" class="mi">d</span><span id="MathJax-Span-78" class="mo">(</span><span id="MathJax-Span-79" class="mi">x</span><span id="MathJax-Span-80" class="mo">,</span><span id="MathJax-Span-81" class="mi">y</span><span id="MathJax-Span-82" class="mo">)</span><span id="MathJax-Span-83" class="mo">=</span><span id="MathJax-Span-84" class="mo">⟨</span><span id="MathJax-Span-85" class="mi">x</span><span id="MathJax-Span-86" class="mo">,</span><span id="MathJax-Span-87" class="mi">y</span><span id="MathJax-Span-88" class="mo">⟩</span></span></span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation">d(x,y)=⟨x,y⟩</span></span>

<span id="MathJax-Element-4-Frame" class="MathJax" style="box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 14px; text-indent: 0px; text-align: center; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;" tabindex="0" role="presentation" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E8;</mo><mi>x</mi><mo>&amp;#x2212;</mo><mi>y</mi><mo>,</mo><mi>x</mi><mo>&amp;#x2212;</mo><mi>y</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E9;</mo></mrow><mo stretchy=&quot;false&quot;>)</mo></math>"><span id="MathJax-Span-89" class="math"><span id="MathJax-Span-90" class="mrow"><span id="MathJax-Span-91" class="mi">d</span><span id="MathJax-Span-92" class="mo">(</span><span id="MathJax-Span-93" class="mi">x</span><span id="MathJax-Span-94" class="mo">,</span><span id="MathJax-Span-95" class="mi">y</span><span id="MathJax-Span-96" class="mo">)</span><span id="MathJax-Span-97" class="mo">=</span><span id="MathJax-Span-98" class="mi">exp</span><span id="MathJax-Span-99" class="mo"></span><span id="MathJax-Span-100" class="mo">(</span><span id="MathJax-Span-101" class="texatom"><span id="MathJax-Span-102" class="mrow"><span id="MathJax-Span-103" class="mo">−</span><span id="MathJax-Span-104" class="mfrac"><span id="MathJax-Span-105" class="mn">1</span><span id="MathJax-Span-106" class="mn">2</span></span><span id="MathJax-Span-107" class="mo">⟨</span><span id="MathJax-Span-108" class="mi">x</span><span id="MathJax-Span-109" class="mo">−</span><span id="MathJax-Span-110" class="mi">y</span><span id="MathJax-Span-111" class="mo">,</span><span id="MathJax-Span-112" class="mi">x</span><span id="MathJax-Span-113" class="mo">−</span><span id="MathJax-Span-114" class="mi">y</span><span id="MathJax-Span-115" class="mo">⟩</span></span></span><span id="MathJax-Span-116" class="mo">)</span></span></span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation">d(x,y)=exp⁡(−12⟨x−y,x−y⟩)</span></span>

<span id="MathJax-Element-5-Frame" class="MathJax" style="box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 14px; text-indent: 0px; text-align: center; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;" tabindex="0" role="presentation" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>d</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>cos</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mfrac><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mo>&amp;#x22C5;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow></mrow><mrow><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo></mrow></mfrac></mrow></math>"><span id="MathJax-Span-117" class="math"><span id="MathJax-Span-118" class="mrow"><span id="MathJax-Span-119" class="mi">d</span><span id="MathJax-Span-120" class="mo">(</span><span id="MathJax-Span-121" class="mi">x</span><span id="MathJax-Span-122" class="mo">,</span><span id="MathJax-Span-123" class="mi">y</span><span id="MathJax-Span-124" class="mo">)</span><span id="MathJax-Span-125" class="mo">=</span><span id="MathJax-Span-126" class="mi">cos</span><span id="MathJax-Span-127" class="mo"></span><span id="MathJax-Span-128" class="mo">(</span><span id="MathJax-Span-129" class="mi">θ</span><span id="MathJax-Span-130" class="mo">)</span><span id="MathJax-Span-131" class="mo">=</span><span id="MathJax-Span-132" class="texatom"><span id="MathJax-Span-133" class="mrow"><span id="MathJax-Span-134" class="mfrac"><span id="MathJax-Span-135" class="mrow"><span id="MathJax-Span-136" class="texatom"><span id="MathJax-Span-137" class="mrow"><span id="MathJax-Span-138" class="mi">x</span></span></span><span id="MathJax-Span-139" class="mo">⋅</span><span id="MathJax-Span-140" class="texatom"><span id="MathJax-Span-141" class="mrow"><span id="MathJax-Span-142" class="mi">y</span></span></span></span><span id="MathJax-Span-143" class="mrow"><span id="MathJax-Span-144" class="mo">∥</span><span id="MathJax-Span-145" class="texatom"><span id="MathJax-Span-146" class="mrow"><span id="MathJax-Span-147" class="mi">x</span></span></span><span id="MathJax-Span-148" class="mo">∥</span><span id="MathJax-Span-149" class="mo">∥</span><span id="MathJax-Span-150" class="texatom"><span id="MathJax-Span-151" class="mrow"><span id="MathJax-Span-152" class="mi">y</span></span></span><span id="MathJax-Span-153" class="mo">∥</span></span></span></span></span></span></span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation">d(x,y)=cos⁡(θ)=x⋅y‖x‖‖y‖</span></span>

<h3 id="Part-1.1-F1-score-and-Distance-Functions">Part 1.1 F1 score and Distance Functions</h3>

Implement the following items in <em>util.py</em>

<pre><code>- f1_score- class Distance    - euclidean_distance    - minkowski_distance    - inner_product_distance    - gaussian_kernel_distance    - cosine distanceand the functions</code></pre>

Simply follow the notes above and to finish all these functions. You are not allowed to call any packages which are already not imported. Please note that all these methods are graded individually so you can take advantage of the grading script to get partial marks for these methods instead of submitting the complete code in one shot.

In [5]:

<pre><span class="k">def</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">real_labels</span><span class="p">,</span> <span class="n">predicted_labels</span><span class="p">):</span>    <span class="sd">"""</span><span class="sd">    Information on F1 score - https://en.wikipedia.org/wiki/F1_score</span><span class="sd">    :param real_labels: List[int]</span><span class="sd">    :param predicted_labels: List[int]</span><span class="sd">    :return: float</span><span class="sd">    """</span><span class="k">class</span> <span class="nc">Distances</span><span class="p">:</span>    <span class="nd">@staticmethod</span>    <span class="k">def</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        Minkowski distance is the generalized version of Euclidean Distance</span><span class="sd">        It is also know as L-p norm (where p&gt;=1) that you have studied in class</span><span class="sd">        For our assignment we need to take p=3</span><span class="sd">        Information on Minkowski distance - https://en.wikipedia.org/wiki/Minkowski_distance</span><span class="sd">        :param point1: List[float]</span><span class="sd">        :param point2: List[float]</span><span class="sd">        :param p: int</span><span class="sd">        :return: float</span><span class="sd">        """</span>    <span class="nd">@staticmethod</span>    <span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        :param point1: List[float]</span><span class="sd">        :param point2: List[float]</span><span class="sd">        :return: float</span><span class="sd">        """</span>    <span class="nd">@staticmethod</span>    <span class="k">def</span> <span class="nf">inner_product_distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        :param point1: List[float]</span><span class="sd">        :param point2: List[float]</span><span class="sd">        :return: float</span><span class="sd">        """</span>    <span class="nd">@staticmethod</span>    <span class="k">def</span> <span class="nf">cosine_similarity_distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">       :param point1: List[float]</span><span class="sd">       :param point2: List[float]</span><span class="sd">       :return: float</span><span class="sd">       """</span>    <span class="nd">@staticmethod</span>    <span class="k">def</span> <span class="nf">gaussian_kernel_distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">       :param point1: List[float]</span><span class="sd">       :param point2: List[float]</span><span class="sd">       :return: float</span><span class="sd">       """</span></pre>

<h3 id="Part-1.2-KNN-class">Part 1.2 KNN class</h3>

Implement the following items in <em>knn.py</em>

<pre><code>- class KNN    - train    - get_k_neighbors    - predict</code></pre>

In [6]:

<pre><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span>        <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        In this function, features is simply training data which is a 2D list with float values.</span><span class="sd">        For example, if the data looks like the following: Student 1 with features age 25, grade 3.8 and labeled as 0,</span><span class="sd">        Student 2 with features age 22, grade 3.0 and labeled as 1, then the feature data would be</span><span class="sd">        [ [25.0, 3.8], [22.0,3.0] ] and the corresponding label would be [0,1]</span><span class="sd">        For KNN, the training process is just loading of training data. Thus, all you need to do in this function</span><span class="sd">        is create some local variable in KNN class to store this data so you can use the data in later process.</span><span class="sd">        :param features: List[List[float]]</span><span class="sd">        :param labels: List[int]</span><span class="sd">        """</span>    <span class="k">def</span> <span class="nf">get_k_neighbors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">point</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        This function takes one single data point and finds k-nearest neighbours in the training set.</span><span class="sd">        You already have your k value, distance function and you just stored all training data in KNN class with the</span><span class="sd">        train function. This function needs to return a list of labels of all k neighours.</span><span class="sd">        :param point: List[float]</span><span class="sd">        :return:  List[int]</span><span class="sd">        """</span>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        This function takes 2D list of test data points, similar to those from train function. Here, you need process</span><span class="sd">        every test data point, reuse the get_k_neighbours function to find the nearest k neighbours for each test</span><span class="sd">        data point, find the majority of labels for these neighbours as the predict label for that testing data point.</span><span class="sd">        Thus, you will get N predicted label for N test data point.</span><span class="sd">        This function need to return a list of predicted labels for all test data points.</span><span class="sd">        :param features: List[List[float]]</span><span class="sd">        :return: List[int]</span><span class="sd">        """</span></pre>

<h3 id="Part-1.3-Hyperparameter-Tuning">Part 1.3 Hyperparameter Tuning</h3>

Implement the following items in <em>knn.py</em>

<pre><code>- class KNN    - train    - get_k_neighbors    - predict</code></pre>

In [12]:

<pre><span class="k">class</span> <span class="nc">HyperparameterTuner</span><span class="p">:</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>        <span class="bp">self</span><span class="o">.</span><span class="n">best_k</span> <span class="o">=</span> <span class="kc">None</span>        <span class="bp">self</span><span class="o">.</span><span class="n">best_distance_function</span> <span class="o">=</span> <span class="kc">None</span>        <span class="bp">self</span><span class="o">.</span><span class="n">best_scaler</span> <span class="o">=</span> <span class="kc">None</span>    <span class="k">def</span> <span class="nf">tuning_without_scaling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distance_funcs</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        :param distance_funcs: dictionary of distance functions you must use to calculate the distance.</span><span class="sd">            Make sure you loop over all distance functions for each data point and each k value.</span><span class="sd">            You can refer to test.py file to see the format in which these functions will be</span><span class="sd">            passed by the grading script</span><span class="sd">        :param x_train: List[List[int]] training data set to train your KNN model</span><span class="sd">        :param y_train: List[int] train labels to train your KNN model</span><span class="sd">        :param x_val:  List[List[int]] Validation data set will be used on your KNN predict function to produce</span><span class="sd">            predicted labels and tune k and distance function.</span><span class="sd">        :param y_val: List[int] validation labels</span><span class="sd">        Find(tune) best k, distance_function and model (an instance of KNN) and assign to self.best_k,</span><span class="sd">        self.best_distance_function and self.best_model respectively.</span><span class="sd">        NOTE: self.best_scaler will be None</span><span class="sd">        NOTE: When there is a tie, choose model based on the following priorities:</span><span class="sd">        Then check distance function  [euclidean &gt; minkowski &gt; gaussian &gt; inner_prod &gt; cosine_dist]</span><span class="sd">        If they have same distance fuction, choose model which has a less k.</span><span class="sd">        """</span></pre>

<h3 id="Part-1.4-Data-Transformation">Part 1.4 Data Transformation</h3>

We are going to add one more step (data transformation) in the data processing part and see how it works. Sometimes, normalization plays an important role to make a machine learning model work. This link might be helpful <a href="https://en.wikipedia.org/wiki/Feature_scaling">https://en.wikipedia.org/wiki/Feature_scaling</a>

Here, we take two different data transformation approaches.

<h4 id="Normalizing-the-feature-vector">Normalizing the feature vector</h4>

This one is simple but some times may work well. Given a feature vector x, the normalized feature vector is given by

If a vector is a all-zero vector, we let the normalized vector also be a all-zero vector.

<h4 id="Min-max-scaling-the-feature-matrix">Min-max scaling the feature matrix</h4>

The above normalization is data independent, that is to say, the output of the normalization function doesn’t depend on rest of the training data. However, sometimes it is helpful to do data dependent normalization. One thing to note is that, when doing data dependent normalization, we can only use training data, as the test data is assumed to be unknown during training (at least for most classification tasks).

The min-max scaling works as follows: after min-max scaling, all values of training data’s feature vectors are in the given range. Note that this doesn’t mean the values of the validation/test data’s features are all in that range, because the validation/test data may have different distribution as the training data.

Implement the functions in the classes NormalizationScaler and MinMaxScaler in utils.py

1.normalize

normalize the feature vector for each sample . For example, if the input features = [[3, 4], [1, -1], [0, 0]], the output should be [[0.6, 0.8], [0.707107, -0.707107], [0, 0]]

2.min_max_scale

normalize the feature vector for each sample . For example, if the input features = [[2, -1], [-1, 5], [0, 0]], the output should be [[1, 0], [0, 1], [0.333333, 0.16667]]

<span id="MathJax-Element-6-Frame" class="MathJax" style="box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 14px; text-indent: 0px; text-align: center; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;" tabindex="0" role="presentation" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>x</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mfrac><mi>x</mi><msqrt><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E8;</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E9;</mo></msqrt></mfrac></math>"><span id="MathJax-Span-154" class="math"><span id="MathJax-Span-155" class="mrow"><span id="MathJax-Span-156" class="msup"><span id="MathJax-Span-157" class="mi">x</span><span id="MathJax-Span-158" class="mo">′</span></span><span id="MathJax-Span-159" class="mo">=</span><span id="MathJax-Span-160" class="mfrac"><span id="MathJax-Span-161" class="mi">x</span><span id="MathJax-Span-162" class="msqrt"><span id="MathJax-Span-163" class="mrow"><span id="MathJax-Span-164" class="mo">⟨</span><span id="MathJax-Span-165" class="mi">x</span><span id="MathJax-Span-166" class="mo">,</span><span id="MathJax-Span-167" class="mi">x</span><span id="MathJax-Span-168" class="mo">⟩</span></span>−−−−−√</span></span></span></span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation">x′=x⟨x,x⟩</span></span>

In [9]:

<pre><span class="k">class</span> <span class="nc">NormalizationScaler</span><span class="p">:</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>        <span class="k">pass</span>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        Normalize features for every sample</span><span class="sd">        Example</span><span class="sd">        features = [[3, 4], [1, -1], [0, 0]]</span><span class="sd">        return [[0.6, 0.8], [0.707107, -0.707107], [0, 0]]</span><span class="sd">        :param features: List[List[float]]</span><span class="sd">        :return: List[List[float]]</span><span class="sd">        """</span><span class="k">class</span> <span class="nc">MinMaxScaler</span><span class="p">:</span>    <span class="sd">"""</span><span class="sd">    Please follow this link to know more about min max scaling</span><span class="sd">    https://en.wikipedia.org/wiki/Feature_scaling</span><span class="sd">    You should keep some states inside the object.</span><span class="sd">    You can assume that the parameter of the first __call__</span><span class="sd">    will be the training set.</span><span class="sd">    Hint: Use a variable to check for first __call__ and only compute</span><span class="sd">            and store min/max in that case.</span><span class="sd">    Note: You may assume the parameters are valid when __call__</span><span class="sd">            is being called the first time (you can find min and max).</span><span class="sd">    Example:</span><span class="sd">        train_features = [[0, 10], [2, 0]]</span><span class="sd">        test_features = [[20, 1]]</span><span class="sd">        scaler1 = MinMaxScale()</span><span class="sd">        train_features_scaled = scaler1(train_features)</span><span class="sd">        # train_features_scaled should be equal to [[0, 1], [1, 0]]</span><span class="sd">        test_features_scaled = scaler1(test_features)</span><span class="sd">        # test_features_scaled should be equal to [[10, 0.1]]</span><span class="sd">        new_scaler = MinMaxScale() # creating a new scaler</span><span class="sd">        _ = new_scaler([[1, 1], [0, 0]]) # new trainfeatures</span><span class="sd">        test_features_scaled = new_scaler(test_features)</span><span class="sd">        # now test_features_scaled should be [[20, 1]]</span><span class="sd">    """</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>        <span class="k">pass</span>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">        normalize the feature vector for each sample . For example,</span><span class="sd">        if the input features = [[2, -1], [-1, 5], [0, 0]],</span><span class="sd">        the output should be [[1, 0], [0, 1], [0.333333, 0.16667]]</span><span class="sd">        :param features: List[List[float]]</span><span class="sd">        :return: List[List[float]]</span><span class="sd">        """</span></pre>

<h4 id="Hyperparameter-Tuning">Hyperparameter Tuning</h4>

This part is similar to Part 1.3 except that before passing your trainig and validation data to KNN model to tune k and distance function, you need to create the normalized data using these two scalers to transform your data, both training and validation. Again, we will use f1-score to compare different models. Here we have 3 hyperparameters i.e. k, distance_function and scaler.

In [11]:

<pre><span class="k">class</span> <span class="nc">HyperparameterTuner</span><span class="p">:</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>        <span class="bp">self</span><span class="o">.</span><span class="n">best_k</span> <span class="o">=</span> <span class="kc">None</span>        <span class="bp">self</span><span class="o">.</span><span class="n">best_distance_function</span> <span class="o">=</span> <span class="kc">None</span>        <span class="bp">self</span><span class="o">.</span><span class="n">best_scaler</span> <span class="o">=</span> <span class="kc">None</span>    <span class="k">def</span> <span class="nf">tuning_with_scaling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distance_funcs</span><span class="p">,</span> <span class="n">scaling_classes</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">):</span>        <span class="sd">"""</span><span class="sd">         :param distance_funcs: dictionary of distance funtions you use to calculate the distance. Make sure you</span><span class="sd">            loop over all distance function for each data point and each k value.</span><span class="sd">            You can refer to test.py file to see the format in which these functions will be</span><span class="sd">            passed by the grading script</span><span class="sd">        :param scaling_classes: dictionary of scalers you will use to normalized your data.</span><span class="sd">        Refer to test.py file to check the format.</span><span class="sd">        :param x_train: List[List[int]] training data set to train your KNN model</span><span class="sd">        :param y_train: List[int] train labels to train your KNN model</span><span class="sd">        :param x_val: List[List[int]] validation data set you will use on your KNN predict function to produce predicted</span><span class="sd">            labels and tune your k, distance function and scaler.</span><span class="sd">        :param y_val: List[int] validation labels</span><span class="sd">        Find(tune) best k, distance_funtion, scaler and model (an instance of KNN) and assign to self.best_k,</span><span class="sd">        self.best_distance_function, self.best_scaler and self.best_model respectively</span><span class="sd">        NOTE: When there is a tie, choose model based on the following priorities:</span><span class="sd">        For normalization, [min_max_scale &gt; normalize];</span><span class="sd">        Then check distance function  [euclidean &gt; minkowski &gt; gaussian &gt; inner_prod &gt; cosine_dist]</span><span class="sd">        If they have same distance function, choose model which has a less k.</span><span class="sd">        """</span></pre>

<h3 id="Use-of-test.py-file">Use of test.py file</h3>

Please make use of test.py file to debug your code and make sure your code is running properly. After you have completed all the classes and functions mentioned above, test.py file will run smoothly and will show a similar output as follows (This is a sample output. your actual output values might vary):

x_train shape = (242, 14) y_train shape = (242,)

<h4 id="Without-Scaling">Without Scaling</h4>

k = 1 distance function = euclidean

<h4 id="With-Scaling">With Scaling</h4>

k = 23 distance function = cosine_dist scaler = min_max_scale